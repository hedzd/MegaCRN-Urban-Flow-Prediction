{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "    def __init__(self, xs, ys, batch_size, pad_with_last_sample=True, shuffle=False):\n",
    "        \"\"\"\n",
    "\n",
    "        :param xs:\n",
    "        :param ys:\n",
    "        :param batch_size:\n",
    "        :param pad_with_last_sample: pad with the last sample to make number of samples divisible to batch_size.\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.current_ind = 0\n",
    "        if pad_with_last_sample:\n",
    "            num_padding = (batch_size - (len(xs) % batch_size)) % batch_size\n",
    "            x_padding = np.repeat(xs[-1:], num_padding, axis=0)\n",
    "            y_padding = np.repeat(ys[-1:], num_padding, axis=0)\n",
    "            xs = np.concatenate([xs, x_padding], axis=0)\n",
    "            ys = np.concatenate([ys, y_padding], axis=0)\n",
    "        self.size = len(xs)\n",
    "        self.num_batch = int(self.size // self.batch_size)\n",
    "        if shuffle:\n",
    "            permutation = np.random.permutation(self.size)\n",
    "            xs, ys = xs[permutation], ys[permutation]\n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "\n",
    "    def get_iterator(self):\n",
    "        self.current_ind = 0\n",
    "\n",
    "        def _wrapper():\n",
    "            while self.current_ind < self.num_batch:\n",
    "                start_ind = self.batch_size * self.current_ind\n",
    "                end_ind = min(self.size, self.batch_size * (self.current_ind + 1))\n",
    "                x_i = self.xs[start_ind: end_ind, ...]\n",
    "                y_i = self.ys[start_ind: end_ind, ...]\n",
    "                yield (x_i, y_i)\n",
    "                self.current_ind += 1\n",
    "\n",
    "        return _wrapper()\n",
    "    \n",
    "class StandardScaler():\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def transform(self, data):\n",
    "        return (data - self.mean) / self.std\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return (data * self.std) + self.mean\n",
    "    \n",
    "def getTimestamp(data):\n",
    "    num_samples, num_nodes = data.shape\n",
    "    time_ind = (data.index.values - data.index.values.astype(\"datetime64[D]\")) / np.timedelta64(1, \"D\")\n",
    "    time_in_day = np.tile(time_ind, [num_nodes,1]).transpose((1, 0))\n",
    "    return time_in_day\n",
    "\n",
    "def getDayTimestamp(data):\n",
    "    # 288 timeslots each day for dataset has 5 minutes time interval.\n",
    "    df = pd.DataFrame({'timestamp':data.index.values})\n",
    "    df['weekdaytime'] = df['timestamp'].dt.weekday * 288 + (df['timestamp'].dt.hour * 60 + df['timestamp'].dt.minute)//5\n",
    "    df['weekdaytime'] = df['weekdaytime'] / df['weekdaytime'].max()\n",
    "    num_samples, num_nodes = data.shape\n",
    "    time_ind = df['weekdaytime'].values\n",
    "    time_ind_node = np.tile(time_ind, [num_nodes,1]).transpose((1, 0))\n",
    "    return time_ind_node\n",
    "\n",
    "def getDayTimestamp_(start, end, freq, num_nodes):\n",
    "    # 288 timeslots each day for dataset has 5 minutes time interval.\n",
    "    df = pd.DataFrame({'timestamp':pd.date_range(start=start, end=end, freq=freq)})\n",
    "    df['weekdaytime'] = df['timestamp'].dt.weekday * 288 + (df['timestamp'].dt.hour * 60 + df['timestamp'].dt.minute)//5\n",
    "    df['weekdaytime'] = df['weekdaytime'] / df['weekdaytime'].max()\n",
    "    time_ind = df['weekdaytime'].values\n",
    "    time_ind_node = np.tile(time_ind, [num_nodes, 1]).transpose((1, 0))\n",
    "    return time_ind_node\n",
    "\n",
    "def masked_mse(preds, labels, null_val=1e-3):\n",
    "    if np.isnan(null_val):\n",
    "        mask = ~torch.isnan(labels)\n",
    "    else:\n",
    "        mask = (labels > null_val)\n",
    "    mask = mask.float()\n",
    "    mask /= torch.mean((mask))\n",
    "    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
    "    loss = (preds-labels)**2\n",
    "    loss = loss * mask\n",
    "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
    "    return torch.mean(loss)\n",
    "\n",
    "def masked_rmse(preds, labels, null_val=1e-3):\n",
    "    return torch.sqrt(masked_mse(preds=preds, labels=labels, null_val=null_val))\n",
    "\n",
    "\n",
    "def masked_mae(preds, labels, null_val=1e-3):\n",
    "    if np.isnan(null_val):\n",
    "        mask = ~torch.isnan(labels)\n",
    "    else:\n",
    "        mask = (labels > null_val)\n",
    "    mask = mask.float()\n",
    "    mask /=  torch.mean((mask))\n",
    "    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
    "    loss = torch.abs(preds-labels)\n",
    "    loss = loss * mask\n",
    "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
    "    return torch.mean(loss)\n",
    "\n",
    "\n",
    "def masked_mape(preds, labels, null_val=1e-3):\n",
    "    if np.isnan(null_val):\n",
    "        mask = ~torch.isnan(labels)\n",
    "    else:\n",
    "        mask = (labels > null_val)\n",
    "    mask = mask.float()\n",
    "    mask /=  torch.mean((mask))\n",
    "    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
    "    loss = torch.abs(preds-labels)/labels\n",
    "    loss = loss * mask\n",
    "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
    "    return torch.mean(loss)\n",
    "\n",
    "# DCRNN\n",
    "def masked_mae_loss(y_pred, y_true):\n",
    "    mask = (y_true != 0).float()\n",
    "    mask /= mask.mean()\n",
    "    loss = torch.abs(y_pred - y_true)\n",
    "    loss = loss * mask\n",
    "    # trick for nans: https://discuss.pytorch.org/t/how-to-set-nan-in-tensor-to-0/3918/3\n",
    "    loss[loss != loss] = 0\n",
    "    return loss.mean()\n",
    "\n",
    "def masked_mape_loss(y_pred, y_true):\n",
    "    mask = (y_true != 0).float()\n",
    "    mask /= mask.mean()\n",
    "    loss = torch.abs(torch.div(y_true - y_pred, y_true))\n",
    "    loss = loss * mask\n",
    "    # trick for nans: https://discuss.pytorch.org/t/how-to-set-nan-in-tensor-to-0/3918/3\n",
    "    loss[loss != loss] = 0\n",
    "    return loss.mean()\n",
    "\n",
    "def masked_rmse_loss(y_pred, y_true):\n",
    "    mask = (y_true != 0).float()\n",
    "    mask /= mask.mean()\n",
    "    loss = torch.pow(y_true - y_pred, 2)\n",
    "    loss = loss * mask\n",
    "    # trick for nans: https://discuss.pytorch.org/t/how-to-set-nan-in-tensor-to-0/3918/3\n",
    "    loss[loss != loss] = 0\n",
    "    return torch.sqrt(loss.mean())\n",
    "\n",
    "def masked_mse_loss(y_pred, y_true):\n",
    "    mask = (y_true != 0).float()\n",
    "    mask /= mask.mean()\n",
    "    loss = torch.pow(y_true - y_pred, 2)\n",
    "    loss = loss * mask\n",
    "    # trick for nans: https://discuss.pytorch.org/t/how-to-set-nan-in-tensor-to-0/3918/3\n",
    "    loss[loss != loss] = 0\n",
    "    return loss.mean()\n",
    "\n",
    "def load_pickle(pickle_file):\n",
    "    try:\n",
    "        with open(pickle_file, 'rb') as f:\n",
    "            pickle_data = pickle.load(f)\n",
    "    except UnicodeDecodeError as e:\n",
    "        with open(pickle_file, 'rb') as f:\n",
    "            pickle_data = pickle.load(f, encoding='latin1')\n",
    "    except Exception as e:\n",
    "        print('Unable to load data ', pickle_file, ':', e)\n",
    "        raise\n",
    "    return pickle_data\n",
    "\n",
    "def print_params(model):\n",
    "    # print trainable params\n",
    "    param_count = 0\n",
    "    print('Trainable parameter list:')\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(name, param.shape, param.numel())\n",
    "            param_count += param.numel()\n",
    "    print(f'\\n In total: {param_count} trainable parameters. \\n')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AGCN(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, cheb_k):\n",
    "        super(AGCN, self).__init__()\n",
    "        self.cheb_k = cheb_k\n",
    "        self.weights = nn.Parameter(torch.FloatTensor(2*cheb_k*dim_in, dim_out)) # 2 is the length of support\n",
    "        self.bias = nn.Parameter(torch.FloatTensor(dim_out))\n",
    "        nn.init.xavier_normal_(self.weights)\n",
    "        nn.init.constant_(self.bias, val=0)\n",
    "        \n",
    "    def forward(self, x, supports):\n",
    "        x_g = []        \n",
    "        support_set = []\n",
    "        for support in supports:\n",
    "            support_ks = [torch.eye(support.shape[0]).to(support.device), support]\n",
    "            for k in range(2, self.cheb_k):\n",
    "                support_ks.append(torch.matmul(2 * support, support_ks[-1]) - support_ks[-2]) \n",
    "            support_set.extend(support_ks)\n",
    "        for support in support_set:\n",
    "            x_g.append(torch.einsum(\"nm,bmc->bnc\", support, x))\n",
    "        x_g = torch.cat(x_g, dim=-1) # B, N, 2 * cheb_k * dim_in\n",
    "        x_gconv = torch.einsum('bni,io->bno', x_g, self.weights) + self.bias  # b, N, dim_out\n",
    "        return x_gconv\n",
    "\n",
    "class AGCRNCell(nn.Module):\n",
    "    def __init__(self, node_num, dim_in, dim_out, cheb_k):\n",
    "        super(AGCRNCell, self).__init__()\n",
    "        self.node_num = node_num\n",
    "        self.hidden_dim = dim_out\n",
    "        self.gate = AGCN(dim_in+self.hidden_dim, 2*dim_out, cheb_k)\n",
    "        self.update = AGCN(dim_in+self.hidden_dim, dim_out, cheb_k)\n",
    "\n",
    "    def forward(self, x, state, supports):\n",
    "        #x: B, num_nodes, input_dim\n",
    "        #state: B, num_nodes, hidden_dim\n",
    "        state = state.to(x.device)\n",
    "        input_and_state = torch.cat((x, state), dim=-1)\n",
    "        z_r = torch.sigmoid(self.gate(input_and_state, supports))\n",
    "        z, r = torch.split(z_r, self.hidden_dim, dim=-1)\n",
    "        candidate = torch.cat((x, z*state), dim=-1)\n",
    "        hc = torch.tanh(self.update(candidate, supports))\n",
    "        h = r*state + (1-r)*hc\n",
    "        return h\n",
    "\n",
    "    def init_hidden_state(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.node_num, self.hidden_dim)\n",
    "    \n",
    "class ADCRNN_Encoder(nn.Module):\n",
    "    def __init__(self, node_num, dim_in, dim_out, cheb_k, num_layers):\n",
    "        super(ADCRNN_Encoder, self).__init__()\n",
    "        assert num_layers >= 1, 'At least one DCRNN layer in the Encoder.'\n",
    "        self.node_num = node_num\n",
    "        self.input_dim = dim_in\n",
    "        self.num_layers = num_layers\n",
    "        self.dcrnn_cells = nn.ModuleList()\n",
    "        self.dcrnn_cells.append(AGCRNCell(node_num, dim_in, dim_out, cheb_k))\n",
    "        for _ in range(1, num_layers):\n",
    "            self.dcrnn_cells.append(AGCRNCell(node_num, dim_out, dim_out, cheb_k))\n",
    "\n",
    "    def forward(self, x, init_state, supports):\n",
    "        #shape of x: (B, T, N, D), shape of init_state: (num_layers, B, N, hidden_dim)\n",
    "        assert x.shape[2] == self.node_num and x.shape[3] == self.input_dim\n",
    "        seq_length = x.shape[1]\n",
    "        current_inputs = x\n",
    "        output_hidden = []\n",
    "        for i in range(self.num_layers):\n",
    "            state = init_state[i]\n",
    "            inner_states = []\n",
    "            for t in range(seq_length):\n",
    "                state = self.dcrnn_cells[i](current_inputs[:, t, :, :], state, supports)\n",
    "                inner_states.append(state)\n",
    "            output_hidden.append(state)\n",
    "            current_inputs = torch.stack(inner_states, dim=1)\n",
    "        #current_inputs: the outputs of last layer: (B, T, N, hidden_dim)\n",
    "        #last_state: (B, N, hidden_dim)\n",
    "        #output_hidden: the last state for each layer: (num_layers, B, N, hidden_dim)\n",
    "        #return current_inputs, torch.stack(output_hidden, dim=0)\n",
    "        return current_inputs, output_hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.dcrnn_cells[i].init_hidden_state(batch_size))\n",
    "        return init_states\n",
    "\n",
    "class ADCRNN_Decoder(nn.Module):\n",
    "    def __init__(self, node_num, dim_in, dim_out, cheb_k, num_layers):\n",
    "        super(ADCRNN_Decoder, self).__init__()\n",
    "        assert num_layers >= 1, 'At least one DCRNN layer in the Decoder.'\n",
    "        self.node_num = node_num\n",
    "        self.input_dim = dim_in\n",
    "        self.num_layers = num_layers\n",
    "        self.dcrnn_cells = nn.ModuleList()\n",
    "        self.dcrnn_cells.append(AGCRNCell(node_num, dim_in, dim_out, cheb_k))\n",
    "        for _ in range(1, num_layers):\n",
    "            self.dcrnn_cells.append(AGCRNCell(node_num, dim_out, dim_out, cheb_k))\n",
    "\n",
    "    def forward(self, xt, init_state, supports):\n",
    "        # xt: (B, N, D)\n",
    "        # init_state: (num_layers, B, N, hidden_dim)\n",
    "        assert xt.shape[1] == self.node_num and xt.shape[2] == self.input_dim\n",
    "        current_inputs = xt\n",
    "        output_hidden = []\n",
    "        for i in range(self.num_layers):\n",
    "            state = self.dcrnn_cells[i](current_inputs, init_state[i], supports)\n",
    "            output_hidden.append(state)\n",
    "            current_inputs = state\n",
    "        return current_inputs, output_hidden\n",
    "\n",
    "\n",
    "class MegaCRN(nn.Module):\n",
    "    def __init__(self, num_nodes, input_dim, output_dim, horizon, rnn_units, num_layers=1, cheb_k=3,\n",
    "                 ycov_dim=1, mem_num=20, mem_dim=64, cl_decay_steps=2000, use_curriculum_learning=True):\n",
    "        super(MegaCRN, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.input_dim = input_dim\n",
    "        self.rnn_units = rnn_units\n",
    "        self.output_dim = output_dim\n",
    "        self.horizon = horizon\n",
    "        self.num_layers = num_layers\n",
    "        self.cheb_k = cheb_k\n",
    "        self.ycov_dim = ycov_dim\n",
    "        self.cl_decay_steps = cl_decay_steps\n",
    "        self.use_curriculum_learning = use_curriculum_learning\n",
    "        \n",
    "        # memory\n",
    "        self.mem_num = mem_num\n",
    "        self.mem_dim = mem_dim\n",
    "        self.memory = self.construct_memory()\n",
    "\n",
    "        # encoder\n",
    "        self.encoder = ADCRNN_Encoder(self.num_nodes, self.input_dim, self.rnn_units, self.cheb_k, self.num_layers)\n",
    "        \n",
    "        # deocoder\n",
    "        self.decoder_dim = self.rnn_units + self.mem_dim\n",
    "        self.decoder = ADCRNN_Decoder(self.num_nodes, self.output_dim + self.ycov_dim, self.decoder_dim, self.cheb_k, self.num_layers)\n",
    "\n",
    "        # output\n",
    "        self.proj = nn.Sequential(nn.Linear(self.decoder_dim, self.output_dim, bias=True))\n",
    "    \n",
    "    def compute_sampling_threshold(self, batches_seen):\n",
    "        return self.cl_decay_steps / (self.cl_decay_steps + np.exp(batches_seen / self.cl_decay_steps))\n",
    "\n",
    "    def construct_memory(self):\n",
    "        memory_dict = nn.ParameterDict()\n",
    "        memory_dict['Memory'] = nn.Parameter(torch.randn(self.mem_num, self.mem_dim), requires_grad=True)     # (M, d)\n",
    "        memory_dict['Wq'] = nn.Parameter(torch.randn(self.rnn_units, self.mem_dim), requires_grad=True)    # project to query\n",
    "        memory_dict['We1'] = nn.Parameter(torch.randn(self.num_nodes, self.mem_num), requires_grad=True) # project memory to embedding\n",
    "        memory_dict['We2'] = nn.Parameter(torch.randn(self.num_nodes, self.mem_num), requires_grad=True) # project memory to embedding\n",
    "        for param in memory_dict.values():\n",
    "            nn.init.xavier_normal_(param)\n",
    "        return memory_dict\n",
    "    \n",
    "    def query_memory(self, h_t:torch.Tensor):\n",
    "        query = torch.matmul(h_t, self.memory['Wq'])     # (B, N, d)\n",
    "        att_score = torch.softmax(torch.matmul(query, self.memory['Memory'].t()), dim=-1)         # alpha: (B, N, M)\n",
    "        value = torch.matmul(att_score, self.memory['Memory'])     # (B, N, d)\n",
    "        _, ind = torch.topk(att_score, k=2, dim=-1)\n",
    "        pos = self.memory['Memory'][ind[:, :, 0]] # B, N, d\n",
    "        neg = self.memory['Memory'][ind[:, :, 1]] # B, N, d\n",
    "        return value, query, pos, neg\n",
    "            \n",
    "    def forward(self, x, y_cov, labels=None, batches_seen=None):\n",
    "        node_embeddings1 = torch.matmul(self.memory['We1'], self.memory['Memory'])\n",
    "        node_embeddings2 = torch.matmul(self.memory['We2'], self.memory['Memory'])\n",
    "        g1 = F.softmax(F.relu(torch.mm(node_embeddings1, node_embeddings2.T)), dim=-1)\n",
    "        g2 = F.softmax(F.relu(torch.mm(node_embeddings2, node_embeddings1.T)), dim=-1)\n",
    "        supports = [g1, g2]\n",
    "        init_state = self.encoder.init_hidden(x.shape[0])\n",
    "        h_en, state_en = self.encoder(x, init_state, supports) # B, T, N, hidden\n",
    "        h_t = h_en[:, -1, :, :] # B, N, hidden (last state)        \n",
    "        \n",
    "        h_att, query, pos, neg = self.query_memory(h_t)\n",
    "        h_t = torch.cat([h_t, h_att], dim=-1)\n",
    "        \n",
    "        ht_list = [h_t]*self.num_layers\n",
    "        go = torch.zeros((x.shape[0], self.num_nodes, self.output_dim), device=x.device)\n",
    "        out = []\n",
    "        for t in range(self.horizon):\n",
    "            h_de, ht_list = self.decoder(torch.cat([go, y_cov[:, t, ...]], dim=-1), ht_list, supports)\n",
    "            go = self.proj(h_de)\n",
    "            out.append(go)\n",
    "            if self.training and self.use_curriculum_learning:\n",
    "                c = np.random.uniform(0, 1)\n",
    "                if c < self.compute_sampling_threshold(batches_seen):\n",
    "                    go = labels[:, t, ...]\n",
    "        output = torch.stack(out, dim=1)\n",
    "        \n",
    "        return output, h_att, query, pos, neg\n",
    "\n",
    "def print_params(model):\n",
    "    # print trainable params\n",
    "    param_count = 0\n",
    "    print('Trainable parameter list:')\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(name, param.shape, param.numel())\n",
    "            param_count += param.numel()\n",
    "    print(f'In total: {param_count} trainable parameters. \\n')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model(model):\n",
    "    param_count = 0\n",
    "    logger.info('Trainable parameter list:')\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(name, param.shape, param.numel())\n",
    "            param_count += param.numel()\n",
    "    logger.info(f'In total: {param_count} trainable parameters.')\n",
    "    return\n",
    "\n",
    "def get_model():  \n",
    "    model = MegaCRN(num_nodes=args.num_nodes, input_dim=args.input_dim, output_dim=args.output_dim, horizon=args.horizon, \n",
    "                    rnn_units=args.rnn_units, num_layers=args.num_rnn_layers, mem_num=args.mem_num, mem_dim=args.mem_dim, \n",
    "                    cheb_k = args.max_diffusion_step, cl_decay_steps=args.cl_decay_steps, use_curriculum_learning=args.use_curriculum_learning).to(device)\n",
    "    return model\n",
    "\n",
    "def prepare_x_y(x, y):\n",
    "    \"\"\"\n",
    "    :param x: shape (batch_size, seq_len, num_sensor, input_dim)\n",
    "    :param y: shape (batch_size, horizon, num_sensor, input_dim)\n",
    "    :return1: x shape (seq_len, batch_size, num_sensor, input_dim)\n",
    "              y shape (horizon, batch_size, num_sensor, input_dim)\n",
    "    :return2: x: shape (seq_len, batch_size, num_sensor * input_dim)\n",
    "              y: shape (horizon, batch_size, num_sensor * output_dim)\n",
    "    \"\"\"\n",
    "    x0 = x[..., :args.input_dim]\n",
    "    y0 = y[..., :args.output_dim]\n",
    "    y1 = y[..., args.output_dim:]\n",
    "    x0 = torch.from_numpy(x0).float()\n",
    "    y0 = torch.from_numpy(y0).float()\n",
    "    y1 = torch.from_numpy(y1).float()\n",
    "    return x0.to(device), y0.to(device), y1.to(device) # x, y, y_cov\n",
    "    \n",
    "def evaluate(model, mode):\n",
    "    with torch.no_grad():\n",
    "        model = model.eval()\n",
    "        data_iter =  data[f'{mode}_loader'].get_iterator()\n",
    "        losses, ys_true, ys_pred = [], [], []\n",
    "        for x, y in data_iter:\n",
    "            x, y, ycov = prepare_x_y(x, y)\n",
    "            output, h_att, query, pos, neg = model(x, ycov)\n",
    "            y_pred = scaler.inverse_transform(output)\n",
    "            y_true = scaler.inverse_transform(y)\n",
    "            loss1 = masked_mae_loss(y_pred, y_true) # masked_mae_loss(y_pred, y_true)\n",
    "            separate_loss = nn.TripletMarginLoss(margin=1.0)\n",
    "            compact_loss = nn.MSELoss()\n",
    "            loss2 = separate_loss(query, pos.detach(), neg.detach())\n",
    "            loss3 = compact_loss(query, pos.detach())\n",
    "            loss = loss1 + args.lamb * loss2 + args.lamb1 * loss3\n",
    "            losses.append(loss.item())\n",
    "            ys_true.append(y_true)\n",
    "            ys_pred.append(y_pred)\n",
    "        mean_loss = np.mean(losses)\n",
    "        y_size = data[f'y_{mode}'].shape[0]\n",
    "        ys_true, ys_pred = torch.cat(ys_true, dim=0)[:y_size], torch.cat(ys_pred, dim=0)[:y_size]\n",
    "\n",
    "        if mode == 'test':\n",
    "            ys_true, ys_pred = ys_true.permute(1, 0, 2, 3), ys_pred.permute(1, 0, 2, 3)\n",
    "            mae = masked_mae_loss(ys_pred, ys_true).item()\n",
    "            mape = masked_mape_loss(ys_pred, ys_true).item()\n",
    "            rmse = masked_rmse_loss(ys_pred, ys_true).item()\n",
    "            mae_3 = masked_mae_loss(ys_pred[2:3], ys_true[2:3]).item()\n",
    "            mape_3 = masked_mape_loss(ys_pred[2:3], ys_true[2:3]).item()\n",
    "            rmse_3 = masked_rmse_loss(ys_pred[2:3], ys_true[2:3]).item()\n",
    "            mae_6 = masked_mae_loss(ys_pred[5:6], ys_true[5:6]).item()\n",
    "            mape_6 = masked_mape_loss(ys_pred[5:6], ys_true[5:6]).item()\n",
    "            rmse_6 = masked_rmse_loss(ys_pred[5:6], ys_true[5:6]).item()\n",
    "            mae_12 = masked_mae_loss(ys_pred[11:12], ys_true[11:12]).item()\n",
    "            mape_12 = masked_mape_loss(ys_pred[11:12], ys_true[11:12]).item()\n",
    "            rmse_12 = masked_rmse_loss(ys_pred[11:12], ys_true[11:12]).item()\n",
    "            logger.info('Horizon overall: mae: {:.4f}, mape: {:.4f}, rmse: {:.4f}'.format(mae, mape, rmse))\n",
    "            logger.info('Horizon 15mins: mae: {:.4f}, mape: {:.4f}, rmse: {:.4f}'.format(mae_3, mape_3, rmse_3))\n",
    "            logger.info('Horizon 30mins: mae: {:.4f}, mape: {:.4f}, rmse: {:.4f}'.format(mae_6, mape_6, rmse_6))\n",
    "            logger.info('Horizon 60mins: mae: {:.4f}, mape: {:.4f}, rmse: {:.4f}'.format(mae_12, mape_12, rmse_12))\n",
    "            ys_true, ys_pred = ys_true.permute(1, 0, 2, 3), ys_pred.permute(1, 0, 2, 3)\n",
    "            \n",
    "        return mean_loss, ys_true, ys_pred\n",
    "        \n",
    "def traintest_model():  \n",
    "    model = get_model()\n",
    "    print_model(model)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, eps=args.epsilon)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=args.steps, gamma=args.lr_decay_ratio)\n",
    "    min_val_loss = float('inf')\n",
    "    wait = 0\n",
    "    batches_seen = 0\n",
    "    for epoch_num in range(args.epochs):\n",
    "        start_time = time.time()\n",
    "        model = model.train()\n",
    "        data_iter = data['train_loader'].get_iterator()\n",
    "        losses = []\n",
    "        for x, y in data_iter:\n",
    "            optimizer.zero_grad()\n",
    "            x, y, ycov = prepare_x_y(x, y)\n",
    "            output, h_att, query, pos, neg = model(x, ycov, y, batches_seen)\n",
    "            y_pred = scaler.inverse_transform(output)\n",
    "            y_true = scaler.inverse_transform(y)\n",
    "            loss1 = masked_mae_loss(y_pred, y_true)\n",
    "            separate_loss = nn.TripletMarginLoss(margin=1.0)\n",
    "            compact_loss = nn.MSELoss()\n",
    "            loss2 = separate_loss(query, pos.detach(), neg.detach())\n",
    "            loss3 = compact_loss(query, pos.detach())\n",
    "            loss = loss1 + args.lamb * loss2 + args.lamb1 * loss3\n",
    "            losses.append(loss.item())\n",
    "            batches_seen += 1\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "            optimizer.step()\n",
    "        train_loss = np.mean(losses)\n",
    "        lr_scheduler.step()\n",
    "        val_loss, _, _ = evaluate(model, 'val')\n",
    "        end_time2 = time.time()\n",
    "        message = 'Epoch [{}/{}] ({}) train_loss: {:.4f}, val_loss: {:.4f}, lr: {:.6f}, {:.1f}s'.format(epoch_num + 1, \n",
    "                   args.epochs, batches_seen, train_loss, val_loss, optimizer.param_groups[0]['lr'], (end_time2 - start_time))\n",
    "        logger.info(message)\n",
    "        test_loss, _, _ = evaluate(model, 'test')\n",
    "        \n",
    "        if val_loss < min_val_loss:\n",
    "            wait = 0\n",
    "            min_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), modelpt_path)\n",
    "        elif val_loss >= min_val_loss:\n",
    "            wait += 1\n",
    "            if wait == args.patience:\n",
    "                logger.info('Early stopping at epoch: %d' % epoch_num)\n",
    "                break\n",
    "    \n",
    "    logger.info('=' * 35 + 'Best model performance' + '=' * 35)\n",
    "    model = get_model()\n",
    "    model.load_state_dict(torch.load(modelpt_path))\n",
    "    test_loss, _, _ = evaluate(model, 'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', type=str, choices=['METRLA', 'PEMSBAY'], default='METRLA', help='which dataset to run')\n",
    "parser.add_argument('--trainval_ratio', type=float, default=0.8, help='the ratio of training and validation data among the total')\n",
    "parser.add_argument('--val_ratio', type=float, default=0.125, help='the ratio of validation data among the trainval ratio')\n",
    "parser.add_argument('--num_nodes', type=int, default=207, help='num_nodes')\n",
    "parser.add_argument('--seq_len', type=int, default=12, help='input sequence length')\n",
    "parser.add_argument('--horizon', type=int, default=12, help='output sequence length')\n",
    "parser.add_argument('--input_dim', type=int, default=1, help='number of input channel')\n",
    "parser.add_argument('--output_dim', type=int, default=1, help='number of output channel')\n",
    "parser.add_argument('--max_diffusion_step', type=int, default=3, help='max diffusion step or Cheb K')\n",
    "parser.add_argument('--num_rnn_layers', type=int, default=1, help='number of rnn layers')\n",
    "parser.add_argument('--rnn_units', type=int, default=64, help='number of rnn units')\n",
    "parser.add_argument('--mem_num', type=int, default=20, help='number of meta-nodes/prototypes')\n",
    "parser.add_argument('--mem_dim', type=int, default=64, help='dimension of meta-nodes/prototypes')\n",
    "parser.add_argument(\"--loss\", type=str, default='mask_mae_loss', help=\"mask_mae_loss\")\n",
    "parser.add_argument('--lamb', type=float, default=0.01, help='lamb value for separate loss')\n",
    "parser.add_argument('--lamb1', type=float, default=0.01, help='lamb1 value for compact loss')\n",
    "parser.add_argument(\"--epochs\", type=int, default=200, help=\"number of epochs of training\")\n",
    "parser.add_argument(\"--patience\", type=int, default=20, help=\"patience used for early stop\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64, help=\"size of the batches\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.01, help=\"base learning rate\")\n",
    "parser.add_argument(\"--steps\", type=eval, default=[50, 100], help=\"steps\")\n",
    "parser.add_argument(\"--lr_decay_ratio\", type=float, default=0.1, help=\"lr_decay_ratio\")\n",
    "parser.add_argument(\"--epsilon\", type=float, default=1e-3, help=\"optimizer epsilon\")\n",
    "parser.add_argument(\"--max_grad_norm\", type=int, default=5, help=\"max_grad_norm\")\n",
    "parser.add_argument(\"--use_curriculum_learning\", type=eval, choices=[True, False], default='True', help=\"use_curriculum_learning\")\n",
    "parser.add_argument(\"--cl_decay_steps\", type=int, default=2000, help=\"cl_decay_steps\")\n",
    "parser.add_argument('--test_every_n_epochs', type=int, default=5, help='test_every_n_epochs')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='which gpu to use')\n",
    "parser.add_argument('--seed', type=int, default=100, help='random seed.')\n",
    "args = parser.parse_args()\n",
    "        \n",
    "if args.dataset == 'METRLA':\n",
    "    data_path = 'metr-la.h5'\n",
    "    args.num_nodes = 207\n",
    "elif args.dataset == 'YT':\n",
    "    data_path = f'pickup_nyc_yt.csv'\n",
    "    args.num_nodes = 265\n",
    "else:\n",
    "    pass # including more datasets in the future   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'MegaCRN'\n",
    "timestring = time.strftime('%Y%m%d%H%M%S', time.localtime())\n",
    "path = f'../save/{args.dataset}_{model_name}_{timestring}'\n",
    "logging_path = f'{path}/{model_name}_{timestring}_logging.txt'\n",
    "score_path = f'{path}/{model_name}_{timestring}_scores.txt'\n",
    "epochlog_path = f'{path}/{model_name}_{timestring}_epochlog.txt'\n",
    "modelpt_path = f'{path}/{model_name}_{timestring}.pt'\n",
    "if not os.path.exists(path): os.makedirs(path)\n",
    "shutil.copy2(sys.argv[0], path)\n",
    "shutil.copy2(f'{model_name}.py', path)\n",
    "shutil.copy2('utils.py', path)\n",
    "    \n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(level = logging.INFO)\n",
    "class MyFormatter(logging.Formatter):\n",
    "    def format(self, record):\n",
    "        spliter = ' '\n",
    "        record.msg = str(record.msg) + spliter + spliter.join(map(str, record.args))\n",
    "        record.args = tuple() # set empty to args\n",
    "        return super().format(record)\n",
    "formatter = MyFormatter()\n",
    "handler = logging.FileHandler(logging_path, mode='a')\n",
    "handler.setLevel(logging.INFO)\n",
    "handler.setFormatter(formatter)\n",
    "console = logging.StreamHandler()\n",
    "console.setLevel(logging.INFO)\n",
    "console.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "logger.addHandler(console)\n",
    "\n",
    "logger.info('model', model_name)\n",
    "logger.info('dataset', args.dataset)\n",
    "logger.info('trainval_ratio', args.trainval_ratio)\n",
    "logger.info('val_ratio', args.val_ratio)\n",
    "logger.info('num_nodes', args.num_nodes)\n",
    "logger.info('seq_len', args.seq_len)\n",
    "logger.info('horizon', args.horizon)\n",
    "logger.info('input_dim', args.input_dim)\n",
    "logger.info('output_dim', args.output_dim)\n",
    "logger.info('num_rnn_layers', args.num_rnn_layers)\n",
    "logger.info('rnn_units', args.rnn_units)\n",
    "logger.info('max_diffusion_step', args.max_diffusion_step)\n",
    "logger.info('mem_num', args.mem_num)\n",
    "logger.info('mem_dim', args.mem_dim)\n",
    "logger.info('loss', args.loss)\n",
    "logger.info('separate loss lamb', args.lamb)\n",
    "logger.info('compact loss lamb1', args.lamb1)\n",
    "logger.info('batch_size', args.batch_size)\n",
    "logger.info('epochs', args.epochs)\n",
    "logger.info('patience', args.patience)\n",
    "logger.info('lr', args.lr)\n",
    "logger.info('epsilon', args.epsilon)\n",
    "logger.info('steps', args.steps)\n",
    "logger.info('lr_decay_ratio', args.lr_decay_ratio)\n",
    "logger.info('use_curriculum_learning', args.use_curriculum_learning)\n",
    "\n",
    "cpu_num = 1\n",
    "\"\"\"os.environ ['OMP_NUM_THREADS'] = str(cpu_num)\n",
    "os.environ ['OPENBLAS_NUM_THREADS'] = str(cpu_num)\n",
    "os.environ ['MKL_NUM_THREADS'] = str(cpu_num)\n",
    "os.environ ['VECLIB_MAXIMUM_THREADS'] = str(cpu_num)\n",
    "os.environ ['NUMEXPR_NUM_THREADS'] = str(cpu_num)\"\"\"\n",
    "torch.set_num_threads(cpu_num)\n",
    "device = torch.device(\"cuda:{}\".format(args.gpu)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# Please comment the following three lines for running experiments multiple times.\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed(args.seed)\n",
    "#####################################################################################################\n",
    "\n",
    "data = {}\n",
    "for category in ['train', 'val', 'test']:\n",
    "    cat_data = np.load(os.path.join(f'../{args.dataset}', category + '.npz'))\n",
    "    data['x_' + category] = cat_data['x']\n",
    "    data['y_' + category] = cat_data['y']\n",
    "scaler = StandardScaler(mean=data['x_train'][..., 0].mean(), std=data['x_train'][..., 0].std())\n",
    "for category in ['train', 'val', 'test']:\n",
    "    data['x_' + category][..., 0] = scaler.transform(data['x_' + category][..., 0])\n",
    "    data['y_' + category][..., 0] = scaler.transform(data['y_' + category][..., 0])\n",
    "data['train_loader'] = DataLoader(data['x_train'], data['y_train'], args.batch_size, shuffle=True)\n",
    "data['val_loader'] = DataLoader(data['x_val'], data['y_val'], args.batch_size, shuffle=False)\n",
    "data['test_loader'] = DataLoader(data['x_test'], data['y_test'], args.batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    logger.info(args.dataset, 'training and testing started', time.ctime())\n",
    "    logger.info('train xs.shape, ys.shape', data['x_train'].shape, data['y_train'].shape)\n",
    "    logger.info('val xs.shape, ys.shape', data['x_val'].shape, data['y_val'].shape)\n",
    "    logger.info('test xs.shape, ys.shape', data['x_test'].shape, data['y_test'].shape)\n",
    "    traintest_model()\n",
    "    logger.info(args.dataset, 'training and testing ended', time.ctime())\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
