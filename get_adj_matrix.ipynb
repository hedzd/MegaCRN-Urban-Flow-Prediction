{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_parquet('presentation_data/yellow_tripdata_2023-01.parquet', engine='pyarrow')\n",
    "df2 = pd.read_parquet('presentation_data/yellow_tripdata_2023-02.parquet', engine='pyarrow')\n",
    "df3 = pd.read_parquet('presentation_data/yellow_tripdata_2023-03.parquet', engine='pyarrow')\n",
    "df4 = pd.read_parquet('presentation_data/yellow_tripdata_2023-04.parquet', engine='pyarrow')\n",
    "df5 = pd.read_parquet('presentation_data/yellow_tripdata_2023-05.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df1, df2, df3, df4, df5], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265\n"
     ]
    }
   ],
   "source": [
    "print(max(df_all['PULocationID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16186386, 20)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime',\n",
       "       'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag',\n",
       "       'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra',\n",
       "       'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge',\n",
       "       'total_amount', 'congestion_surcharge', 'airport_fee', 'Airport_fee'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(265, 265)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_tab = pd.crosstab(df_all['PULocationID'], df_all['DOLocationID'], normalize='all')\n",
    "\n",
    "# Convert column names to integers\n",
    "columns = sorted(map(int, cross_tab.columns))\n",
    "\n",
    "# Find missing columns and insert them with zeros\n",
    "for prev, curr in zip(columns, columns[1:]):\n",
    "    if curr - prev > 1:\n",
    "        missing_columns = [str(col) for col in range(prev + 1, curr)]\n",
    "        for col in missing_columns:\n",
    "            cross_tab[col] = 0\n",
    "\n",
    "# Sort columns to ensure they are in order\n",
    "cross_tab = cross_tab[sorted(cross_tab.columns, key=int)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "index_values = sorted(map(int, cross_tab.index))\n",
    "\n",
    "# Find missing rows and insert them with zeros\n",
    "for prev, curr in zip(index_values, index_values[1:]):\n",
    "    if curr - prev > 1:\n",
    "        missing_rows = [index for index in range(prev + 1, curr)]\n",
    "        for index in missing_rows:\n",
    "            cross_tab.loc[index] = 0\n",
    "\n",
    "# Sort the DataFrame based on index\n",
    "cross_tab = cross_tab.sort_index()\n",
    "\n",
    "cross_tab.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_normalized = cross_tab.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_npy = 'Data/normalized_adj_matrix.npy'\n",
    "np.save(output_file_npy, matrix_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['tpep_pickup_datetime', 'PULocationID']\n",
    "df_all_simpl = df_all[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all.to_csv('Data/df_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nv/yh00rxr94090cfx1pg6840p40000gn/T/ipykernel_88838/4139302126.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_all_simpl['tpep_pickup_datetime'] = pd.to_datetime(df_all_simpl['tpep_pickup_datetime'])\n",
      "/var/folders/nv/yh00rxr94090cfx1pg6840p40000gn/T/ipykernel_88838/4139302126.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_all_simpl['interval'] = df_all_simpl['tpep_pickup_datetime'].dt.floor('30T')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7306, 262)\n",
      "(7306, 265)\n"
     ]
    }
   ],
   "source": [
    "df_all_simpl['tpep_pickup_datetime'] = pd.to_datetime(df_all_simpl['tpep_pickup_datetime'])\n",
    "\n",
    "df_all_simpl['interval'] = df_all_simpl['tpep_pickup_datetime'].dt.floor('30T')\n",
    "\n",
    "grouped_data = df_all_simpl.groupby(['PULocationID', 'interval']).size().reset_index(name='count')\n",
    "\n",
    "\n",
    "taxi_matrix = grouped_data.pivot(index='interval', columns='PULocationID', values='count').fillna(0)\n",
    "\n",
    "\n",
    "taxi_matrix.index = taxi_matrix.index.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "print(taxi_matrix.shape)\n",
    "\n",
    "# Convert column names to integers\n",
    "columns = sorted(map(int, taxi_matrix.columns))\n",
    "\n",
    "# Find missing columns and insert them with zeros\n",
    "for prev, curr in zip(columns, columns[1:]):\n",
    "    if curr - prev > 1:\n",
    "        missing_columns = [str(col) for col in range(prev + 1, curr)]\n",
    "        for col in missing_columns:\n",
    "            taxi_matrix[col] = 0\n",
    "\n",
    "# Sort columns to ensure they are in order\n",
    "taxi_matrix = taxi_matrix[sorted(taxi_matrix.columns, key=int)]\n",
    "print(taxi_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_matrix = taxi_matrix.to_numpy()\n",
    "output_file_npy = 'Data/data_matrix_bench.npy'\n",
    "np.save(output_file_npy, taxi_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 01:03:11.474897: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-12-18 01:03:11.477368: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-12-18 01:03:11.478752: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 7306, 265)]  0           []                               \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  (None, 7306, 64)     84480       ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 265, 265)]   0           []                               \n",
      "                                                                                                  \n",
      " flatten_4 (Flatten)            (None, 467584)       0           ['lstm_2[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten_5 (Flatten)            (None, 70225)        0           ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 537809)       0           ['flatten_4[0][0]',              \n",
      "                                                                  'flatten_5[0][0]']              \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 64)           34419840    ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 265)          17225       ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 34,521,545\n",
      "Trainable params: 34,521,545\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Training data contains 1 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=0.2`. Either provide more data, or a different value for the `validation_split` argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/fiarresga/Desktop/2024/UC/Presentation/get_adj_matrix.ipynb Cell 17\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fiarresga/Desktop/2024/UC/Presentation/get_adj_matrix.ipynb#X22sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# Expand dimensions for batch size\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fiarresga/Desktop/2024/UC/Presentation/get_adj_matrix.ipynb#X22sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m target_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexpand_dims(target_data, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/fiarresga/Desktop/2024/UC/Presentation/get_adj_matrix.ipynb#X22sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(x\u001b[39m=\u001b[39;49m[expanded_time_series_data, np\u001b[39m.\u001b[39;49mtile(adjacency_matrix, (\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m))], y\u001b[39m=\u001b[39;49mtarget_data, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/engine/data_adapter.py:1687\u001b[0m, in \u001b[0;36mtrain_validation_split\u001b[0;34m(arrays, validation_split)\u001b[0m\n\u001b[1;32m   1684\u001b[0m split_at \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(math\u001b[39m.\u001b[39mfloor(batch_dim \u001b[39m*\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m validation_split)))\n\u001b[1;32m   1686\u001b[0m \u001b[39mif\u001b[39;00m split_at \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m split_at \u001b[39m==\u001b[39m batch_dim:\n\u001b[0;32m-> 1687\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1688\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTraining data contains \u001b[39m\u001b[39m{batch_dim}\u001b[39;00m\u001b[39m samples, which is not \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1689\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msufficient to split it into a validation and training set as \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1690\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mspecified by `validation_split=\u001b[39m\u001b[39m{validation_split}\u001b[39;00m\u001b[39m`. Either \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1691\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mprovide more data, or a different value for the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1692\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`validation_split` argument.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1693\u001b[0m             batch_dim\u001b[39m=\u001b[39mbatch_dim, validation_split\u001b[39m=\u001b[39mvalidation_split\n\u001b[1;32m   1694\u001b[0m         )\n\u001b[1;32m   1695\u001b[0m     )\n\u001b[1;32m   1697\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_split\u001b[39m(t, start, end):\n\u001b[1;32m   1698\u001b[0m     \u001b[39mif\u001b[39;00m t \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Training data contains 1 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=0.2`. Either provide more data, or a different value for the `validation_split` argument."
     ]
    }
   ],
   "source": [
    "check = lambda x: 1 if x.weekday() == 4 and x.hour >= 23 or x.weekday() == 5 and x.hour < 5 or x.weekday() == 5 and x.hour >= 23 or x.weekday() == 6 and x.hour < 5 else 0\n",
    "\n",
    "def df_to_X_y2(df, window_size):\n",
    "    df_as_np = df.to_numpy()\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(df_as_np) - window_size):\n",
    "        row = [r for r in df_as_np[i:i+window_size]]\n",
    "        X.append(row)\n",
    "        label = df_as_np[i+window_size][0]\n",
    "        y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def preproc(X):\n",
    "    X[:,:,0] = (X[:,:,0] - training_mean) / training_std\n",
    "    return X\n",
    "\n",
    "def mape(actual, predicted):\n",
    "    mask = actual != 0\n",
    "    return (np.fabs(actual - predicted)/actual)[mask].mean() * 100\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
